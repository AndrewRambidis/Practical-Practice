{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452f2c6a",
   "metadata": {},
   "source": [
    "## Pyro General Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d6f55",
   "metadata": {},
   "source": [
    "In this notebook, I am taking the time to understand the basic workflow and \n",
    "syntax for the PPL Pyro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f350751",
   "metadata": {},
   "source": [
    "In Bayesian analysis, theory usually begins with the following triple:\n",
    "(data, model, prior). From this, we compute a posterior which is then\n",
    "used for further inference tasks. My question becomes: how does Pyro take this\n",
    "data-model-prior story and perform tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39aa3fa",
   "metadata": {},
   "source": [
    "### Pyro has two workflows one can use to go from model to posterior:\n",
    "\n",
    "#### 1. MCMC Workflow\n",
    "1. We define the generative model $p(w, \\mathbf{y}) = p(\\mathbf{y}|w)\\,p(w)$\n",
    "using a python function ```def model(data)``` where inside we define:\n",
    "    1. The prior, \n",
    "    2. The likelihood.\n",
    "2. Set up MCMC sampling:\n",
    "    * MCMC kernel -> HMC, NUTS, etc.\n",
    "    * MCMC driver -> takes user arguements (e.g. number of samples) and performs\n",
    "    MCMC sampling once called to run.\n",
    "    * Extract samples from MCMC driver.\n",
    "\n",
    "\n",
    "#### 2. Variational Inference Workflow\n",
    "TO DO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abfc4ec",
   "metadata": {},
   "source": [
    "### The Key Pyro Primitives\n",
    "\n",
    "1. ``` pyro.sample(name, dist, obs=None) ```\n",
    "\n",
    "    Prior or likelihood draws. \n",
    "    * ```obs=0``` samples prior;\n",
    "    * ```obs=tensor``` samples likelihood.\n",
    "    \n",
    "\n",
    "2. ``` pyro.param(name, init_tensor, constraint=...) ``` \n",
    "\n",
    "    Deterministic learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3dabe",
   "metadata": {},
   "source": [
    "### Practice:\n",
    "Let us put all of this into practice on a basic Beta-Bernoulli coin-flip model\n",
    "performing both workflows: MCMC and Variational Inference using Pyro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f9c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1500/1500 [00:05, 268.22it/s, step size=9.46e-01, acc. prob=0.910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC Output for w:\n",
      "MEAN: 0.6509367823600769,\n",
      "90% CI: tensor([0.5410, 0.7570]).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS, SVI, Trace_ELBO\n",
    "import pyro.optim as optim\n",
    "\n",
    "# Reset internal parameter state \n",
    "# (important to reset params when re-nunning notebooks)\n",
    "pyro.clear_param_store()\n",
    "\n",
    "## Generate coin-flip data\n",
    "# True parameter w\n",
    "true_w = 0.7\n",
    "N = 50\n",
    "# length-N tensor of 0's and 1's indicating our observed flips\n",
    "data = dist.Bernoulli(torch.tensor(true_w)).sample((N,)) # Shape (N,)\n",
    "\n",
    "## Prior and likelihood\n",
    "# Define the generative model p(w, y)\n",
    "def model(data):\n",
    "    # prior on w ~ Beta(alpha0, beta0)\n",
    "    alpha0 = torch.tensor(1.0) \n",
    "    beta0 = torch.tensor(1.0)\n",
    "    # \"w\" is the latent variable name used later to collect posterior samples\n",
    "    w = pyro.sample(\"w\", dist.Beta(alpha0, beta0))\n",
    "\n",
    "    # Likelihood for each observed flip\n",
    "    # pyro.plate declares that the random choices inside are conditionally\n",
    "    # independent and identically distributed over the \"data\" plate of size N\n",
    "    with pyro.plate(\"data\", len(data)):\n",
    "        # Define y_i ~ Bernoulli(w) as the likelihood\n",
    "        # obs=data tells Pyro to condition on the observed data instead of\n",
    "        # sampling new ones. If sampling new ones, we'd get the posterior over w\n",
    "        pyro.sample(\"obs\", dist.Bernoulli(w), obs=data)\n",
    "\n",
    "## MCMC (using NUTS)\n",
    "# Set up a Hamiltonian Monte Carlo kernel using our model as the target\n",
    "nuts_kernel = NUTS(model)\n",
    "# Set up the MCMC driver with num_samples to keep and warmup_steps as burn-in\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=500)\n",
    "# Actually runs the markov chain \n",
    "mcmc.run(data)\n",
    "# Returns a dictionary of tensors keyed by sample site name\n",
    "# For us, we have key: \"w\", value: tensor of shape (num_samples, )\n",
    "posterior_samples_MCMC = mcmc.get_samples()\n",
    "\n",
    "# Extract the samples drawn from the dictionary with key \"w\"\n",
    "w_samples_MCMC = posterior_samples_MCMC[\"w\"]\n",
    "# Posterior summary statistics (mean and 90% CI)\n",
    "w_mean_MCMC = w_samples_MCMC.mean()\n",
    "w_CI_MCMC = w_samples_MCMC.quantile(torch.tensor([0.05, 0.95]))\n",
    "\n",
    "print(f\"MCMC Output for w:\\nMEAN: {w_mean_MCMC},\\n90% CI: {w_CI_MCMC}.\")\n",
    "\n",
    "## Variational Inference (using SVI)\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78daa09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.mode(\n",
       "values=tensor(0.7360),\n",
       "indices=tensor(574))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_mean_MCMC\n",
    "w_samples_MCMC.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ec1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
